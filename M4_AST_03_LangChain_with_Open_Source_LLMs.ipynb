{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prakum14/Testfiles/blob/master/M4_AST_03_LangChain_with_Open_Source_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMNv0S7qBl1Z"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 3: Open Source LLMs with LangChain ðŸ¦œðŸ”—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7nOHNxJqC2X"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* use open source LLMs: **`zephyr-7b-beta`**, **`Mistral-7B-Instruct-v0.2`**,  and **`Llama2`** through HuggingFaceHub with LangChain\n",
        "* understand & use the concept of Prompt template, Memory and output parsers in LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2416218\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"8975485400\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "WBPPuGmBlDIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9b7e6e5-dc75-4750-fc4f-065677c70c9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2416218&recordId=5473\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_03_LangChain_with_Open_Source_LLMs\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    # ipython.magic(\"wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/Acoustic_Extinguisher_Fire_Dataset.xlsx\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXRnNMfnFqF"
      },
      "source": [
        "### Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aKLiNSv6wTr6"
      },
      "outputs": [],
      "source": [
        "# Langchain\n",
        "!pip -q install langchain\n",
        "\n",
        "# Library to communicate with HF hub\n",
        "!pip -q install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UAsnCf4Z9e1K"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BrFlLJv3-_4N"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sgU1wiq7Hw"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u0IydGx_q78h"
      },
      "outputs": [],
      "source": [
        "# Import the 'os' module to interact with the operating system, such as setting environment variables.\n",
        "import os\n",
        "\n",
        "# Import 'getpass' to securely prompt the user for a password or API key without displaying it on the screen.\n",
        "from getpass import getpass\n",
        "\n",
        "# Import 'HuggingFaceEndpoint' from LangChain's community module to connect and interact with Hugging Face's hosted models.\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "# Import 'PromptTemplate' from LangChain to create structured templates for prompting language models.\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBDqFT0kkgH"
      },
      "source": [
        "### **Provide your HuggingFace api key/access token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9TSeu4cq1Jw",
        "outputId": "fc2f5472-aad5-4c37-880f-3b73f1dc8e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HuggingFace access token: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "# Securely prompt the user to enter their Hugging Face access token without displaying it on the screen.\n",
        "pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "# Store the entered token as an environment variable named 'HF_TOKEN'.\n",
        "os.environ[\"HF_TOKEN\"] = pass_token\n",
        "\n",
        "# Also store the token in 'HUGGINGFACEHUB_API_TOKEN', which may be required for authentication with Hugging Face's API.\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n",
        "\n",
        "# Delete the 'pass_token' variable from memory for security reasons after storing it in environment variables.\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgzDSiux-EuO"
      },
      "source": [
        "### **Exploring Open Source LLMs hosted on HuggingFace**\n",
        "\n",
        ">**I.** `HuggingFaceH4/zephyr-7b-beta`\n",
        ">\n",
        ">**II.** `mistralai/Mistral-7B-Instruct-v0.2`\n",
        ">\n",
        ">**III.** `LlaMa2`\n",
        "\n",
        "[LangChain link](https://python.langchain.com/docs/integrations/chat/huggingface) for using Hugging Face LLM's as chat models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVK3LSt6mzKa"
      },
      "source": [
        "### **I.** [**HuggingFaceH4/zephyr-7b-beta**](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QudSou2y-T4i"
      },
      "outputs": [],
      "source": [
        "# Import HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVRB1nztyTUm",
        "outputId": "a4468d66-b2ee-4904-b80a-2f47c139a9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Hugging Face language model endpoint with specific parameters.\n",
        "llm = HuggingFaceEndpoint(\n",
        "    # Specify the repository ID of the model to use. In this case, it's \"zephyr-7b-beta\" from HuggingFaceH4.\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "\n",
        "    # Define the task type for the model, which is \"text-generation\" (used for generating text responses).\n",
        "    task=\"text-generation\",\n",
        "\n",
        "    # Set the maximum number of new tokens the model can generate in response.\n",
        "    max_new_tokens=512,\n",
        "\n",
        "    # Use 'top_k' sampling to limit the number of highest-probability tokens considered at each generation step.\n",
        "    top_k=30,\n",
        "\n",
        "    # Set the temperature, which controls the randomness of the output. A lower value (e.g., 0.1) makes responses more deterministic.\n",
        "    temperature=0.1,\n",
        "\n",
        "    # Apply a repetition penalty to discourage the model from repeating the same words or phrases too frequently.\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrneNiybIFl_",
        "outputId": "cefabc6d-1c9f-4f46-94db-7af1483463cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. **Start with the Basics**:\n",
            "   - Begin by understanding what programming is and why it's important.\n",
            "   - Learn about variables, data types, loops, and functions. These are fundamental concepts in any programming language.\n",
            "   - Codecademy, freeCodeCamp, and W3Schools offer interactive, beginner-friendly tutorials.\n",
            "\n",
            "2. **Choose a Language**:\n",
            "   - Select a programming language that interests you. Popular choices for beginners include Python, JavaScript, and Java.\n",
            "   - Each language has its own syntax and use cases, so choose one that aligns with your goals (e.g., web development, data analysis, mobile app development).\n",
            "\n",
            "3. **Practice Coding Daily**:\n",
            "   - Consistency is key in learning to program. Aim to code for at least 30 minutes to an hour each day.\n",
            "   - Websites like LeetCode, HackerRank, and Exercism provide coding challenges to help you improve your skills.\n",
            "   - Work on personal projects or contribute to open-source projects to gain practical experience.\n",
            "\n",
            "4. **Learn by Doing**:\n",
            "   - Don't just read about programming; apply what you've learned by writing code.\n",
            "   - Break down complex problems into smaller, manageable tasks.\n",
            "   - Debug your code and learn from your mistakes. This is an essential part of the learning process.\n",
            "\n",
            "5. **Join a Community**:\n",
            "   - Engage with other programmers by joining online communities such as Stack Overflow, GitHub, or Reddit (r/learnprogramming, r/coding, etc.).\n",
            "   - Participate in coding bootcamps, workshops, or local meetups to connect with fellow learners and professionals.\n",
            "   - Ask questions, share your work, and learn from others' experiences. Building a network of peers can greatly enhance your learning journey.\n"
          ]
        }
      ],
      "source": [
        "# Invoke the Hugging Face language model with a specific prompt asking for five points on learning programming.\n",
        "response = llm.invoke(\"How to learn programming? give 5 points\")\n",
        "\n",
        "# Print the model's generated response to the console.\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4N14UjkBaws"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "Prompt templates are predefined recipes for generating prompts for language models.\n",
        "\n",
        "A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
        "\n",
        "LangChain provides tooling to create and work with prompt templates.\n",
        "\n",
        "To know more about Prompt template, refer [here](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-l3yx9hQs2"
      },
      "source": [
        "#### **Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NIjgIglFbjDG",
        "outputId": "e45b4b6e-4803-4e3d-c901-eee49c0b65a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about Trump.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Import the PromptTemplate class from LangChain to create structured prompts.\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template using placeholders for dynamic input values.\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "\n",
        "# Format the template by replacing the placeholders with actual values.\n",
        "messages = prompt_template.format(adjective=\"funny\", content=\"Trump\")\n",
        "\n",
        "# Output the formatted prompt string.\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iUOnAu5c_Hqw"
      },
      "outputs": [],
      "source": [
        "# Import the ChatHuggingFace class from LangChain's Hugging Face integration.\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "# Initialize a chat model using the previously defined Hugging Face LLM.\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjM9t0INbjLg",
        "outputId": "85b4796d-8af5-47b2-c257-7862bd228f5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did Donald Trump throw a clock out the window?\n",
            "\n",
            "Because he wanted to see time fly!\n",
            "\n",
            "(Sentiment: irreverent humor, political satire)\n",
            "\n",
            "(Context: This joke is a playful poke at Donald Trump's obsession with time and his famously impatient personality. The humor lies in the unexpected and absurd situation of throwing a clock out of a window to make time go faster.)\n"
          ]
        }
      ],
      "source": [
        "# Invoke the chat model with the formatted prompt message.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rbI1OD_ds3z"
      },
      "source": [
        "**Practice-1 :**\n",
        "Create a prompt template envisioning a situation where you have to pass three parameters, and the language model responds to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek31fbUK1NqT",
        "outputId": "bfdc5f14-7162-4d5a-d0d4-a37205561c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did President Trump decide to move the White House to Mar-a-Lago?\n",
            "\n",
            "Because he thinks that's where foreign leaders go to pay their respects. \n",
            "\n",
            "(Note: Mar-a-Lago is Trump's private estate in Florida where he has previously hosted several foreign leaders.)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODES HERE for above practice exercise\n",
        "# Import the PromptTemplate class from LangChain to create structured prompts.\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template using placeholders for dynamic input values.\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {length}, {adjective} joke about {content}.\"\n",
        ")\n",
        "\n",
        "# Format the template by replacing the placeholders with actual values.\n",
        "messages = prompt_template.format(length= \"short\",adjective=\"funny\", content=\"Trump\")\n",
        "\n",
        "# Output the formatted prompt string.\n",
        "messages\n",
        "\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPeo4FQBhL0l"
      },
      "source": [
        "#### **Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bat0JTd7GrJx"
      },
      "outputs": [],
      "source": [
        "# Import message classes from LangChain's schema module.\n",
        "from langchain.schema import (\n",
        "    HumanMessage,  # Represents a message sent by the human (user) in a conversation.\n",
        "    SystemMessage,  # Represents a message providing system-level instructions or context.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mQe8qxghewsF",
        "outputId": "5fcba183-fbd1-45b1-9106-25ce60197967"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me 5 facts about Tajmahal.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Import the PromptTemplate class from LangChain to create structured prompts.\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template with placeholders for dynamic values.\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me {count} facts about {event_or_place}.\"\n",
        ")\n",
        "\n",
        "# Format the template by replacing placeholders with actual values.\n",
        "user_msg = prompt_template.format(count=5, event_or_place=\"Tajmahal\")\n",
        "\n",
        "# Output the formatted prompt string.\n",
        "user_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GdGDCoQhmuDL"
      },
      "outputs": [],
      "source": [
        "# Create a list of messages to structure a conversation with both system and human roles.\n",
        "messages = [\n",
        "    # SystemMessage provides a directive to the model about its role or behavior.\n",
        "    SystemMessage(content=\"You're a knowledgeable historian\"),\n",
        "\n",
        "    # HumanMessage contains the prompt or query from the user, which is the formatted message.\n",
        "    HumanMessage(content=user_msg),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vCpfJTPCAqGP"
      },
      "outputs": [],
      "source": [
        "# Import the ChatHuggingFace class from LangChain's Hugging Face integration to handle chat interactions.\n",
        "from langchain_huggingface import ChatHuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9kg1GDWsCRbL"
      },
      "outputs": [],
      "source": [
        "# Initialize the ChatHuggingFace model with the previously defined Hugging Face language model (llm).\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "27Iyhp-5DJ5-",
        "outputId": "47afd1f4-5878-4499-fa16-3a2dc029d6c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HuggingFaceH4/zephyr-7b-beta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Access the model identifier of the Hugging Face model used by the ChatHuggingFace instance.\n",
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9niBkmKIDMVI",
        "outputId": "e8a91a07-dd7d-42fa-ab42-8f35d46793fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou're a knowledgeable historian</s>\\n<|user|>\\nTell me 5 facts about Tajmahal.</s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Convert the sequence of messages into a chat prompt that can be processed by the Hugging Face model.\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px6WPyxN2DYj",
        "outputId": "95986167-e329-4bc2-e51d-68a681b35595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You're a knowledgeable historian</s>\n",
            "<|user|>\n",
            "Tell me 5 facts about Tajmahal.</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert the sequence of messages into a chat prompt and print the resulting formatted prompt.\n",
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZduaUIkl-VpI",
        "outputId": "a2f8ca05-0c9c-4b1c-9fee-f5381da49f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The Taj Mahal is a mausoleum located in Agra, India, commissioned by the Mughal emperor Shah Jahan in memory of his wife Mumtaz Mahal.\n",
            "\n",
            "2. Construction of the Taj Mahal began in 1631 and took around 22 years to complete, with over 20,000 workers involved in the project.\n",
            "\n",
            "3. The white marble used to build the Taj Mahal was sourced from Makrana, a town situated in the state of Rajasthan. The marble was then transported to Agra via a network of rivers.\n",
            "\n",
            "4. The Taj Mahal is a symbol of universal love and is an example of Mughal architecture that combines elements of Persian, Ottoman, and Indian design.\n",
            "\n",
            "5. The Taj Mahal is not just a symbol of love and devotion but also an engineering masterpiece. The tomb is situated on a raised platform and is surrounded by four smaller tombs - one each for Shah Jahan's other wives and a son who died in infancy, as well as a highwayman who helped Shah Jahan escape from imprisonment. Additionally, the Taj Mahal is built on a complex system of canals and aqueducts that helped regulate the temperature inside the tomb, making it cooler in summers and warmer in winters.\n"
          ]
        }
      ],
      "source": [
        "# Invoke the chat model with the sequence of messages, sending it to the Hugging Face model for processing.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-gtbS92fRq_"
      },
      "source": [
        "**Practice-2 :**\n",
        "Create a prompt template envisioning a situation where the language model behaves like a particular persona, and the user's query requires information involving three parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO0xoEdC2ZAy",
        "outputId": "41e1d2a9-bf9f-4a62-e441-d790701bf5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Mahatma Gandhi returned to India from South Africa in 1915, inspiring a non-violent struggle for self-rule. Before this, Gandhi had led a successful non-cooperation movement against the oppressive white minority government in South Africa.\n",
            "\n",
            "2. Gandhi played a significant role in India's freedom struggle as early as 1917 in Protest against the Rowlatt Act, aimed at suppressing political dissent. Gandhi's satyagraha cool down campaign led by the Indian National Congress attracted national and global attention, marking a turning point in the non-violent movement for independence.\n",
            "\n",
            "3. Gandhi led the famous Salt Satyagraha movement in 1930, starting with a salt march from his ashram in Sabarmati to the coast of Dandi, Gujarat. His disobedience of the Salt Laws, characterised by collective civil disobedience, became a watershed in Indian history, leading to international condemnation of British imperialism and growing support for India's independence struggle.\n",
            "\n",
            "4. During the Quit India movement of 1942, Gandhi gave a historic \"Do or Die\" speech urging the Indian people to launch a mass civil disobedience movement for total independence, no matter what the cost. Gandhi was promptly arrested with other leaders and held captive for several years - the longest imprisonment of his career - while the British continued supressing India's freedom struggle.\n",
            "\n",
            "5. Gandhi's later years were plagued by his \"Hindu â€“ Muslim Unity\" efforts. To mitigate the communal tensions between Hindus and Muslims, Gandhi organized an All India Muslim Conference In Delhi in 1942, advocating for Hindu-Muslim unity over the formation of Pakistan. Unfortunately, Gandhi's efforts fell short, leading to the partition of India and Pakistan in 1947 and placing Gandhi at the forefront of India's struggle for independence.\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODES HERE for above practice exercise\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template with placeholders for dynamic values.\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me {count} facts about {person_or_place} of {era}.\"\n",
        ")\n",
        "\n",
        "# Format the template by replacing placeholders with actual values.\n",
        "user_msg = prompt_template.format(count=5, person_or_place=\"Mahatma Gandhi\", era=\"pre-independence\")\n",
        "\n",
        "# Output the formatted prompt string.\n",
        "user_msg\n",
        "\n",
        "messages = [\n",
        "    # SystemMessage provides a directive to the model about its role or behavior.\n",
        "    SystemMessage(content=\"You're a knowledgeable historian\"),\n",
        "\n",
        "    # HumanMessage contains the prompt or query from the user, which is the formatted message.\n",
        "    HumanMessage(content=user_msg),\n",
        "]\n",
        "\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USL19U5MKapo"
      },
      "source": [
        "#### **Example-3**\n",
        "\n",
        "The prompt to *chat models* is a list of chat messages.\n",
        "\n",
        "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jsZWlMZEEcth"
      },
      "outputs": [],
      "source": [
        "# Import the ChatPromptTemplate class from LangChain's core prompts module to create structured chat prompts.\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TBLyJ-sFk8Hf"
      },
      "outputs": [],
      "source": [
        "# Create a ChatPromptTemplate using a list of predefined message templates for system, human, and AI interactions.\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # The system message, defining the behavior or persona of the AI.\n",
        "        (\"system\", \"You are a helpful A {persona}.\"),\n",
        "\n",
        "        # The first human message, a simple greeting.\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "\n",
        "        # The AI's response to the human's greeting.\n",
        "        (\"ai\", \"I'm doing well, thanks!\"),\n",
        "\n",
        "        # The second human message, which will be dynamically filled with user input.\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1rr0S0bklaIr"
      },
      "outputs": [],
      "source": [
        "# Define the persona for the AI, which will be used in the system message template.\n",
        "persona = \"\"\"trustworthy friend\"\"\"\n",
        "\n",
        "# Define the user's query, asking for help with understanding a concept.\n",
        "query = \"\"\"\n",
        "I am not able to understand the concept taught in class. \\\n",
        "Could you please suggest something? \\\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\"\n",
        "\n",
        "# Format the messages in the chat template by filling in the placeholders with the defined persona and query.\n",
        "messages = chat_template.format_messages(persona=persona, user_input=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoEgU1O5LlK7",
        "outputId": "dba6e196-0e1a-4603-e2fb-dab2adb06734"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful A trustworthy friend.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='\\nI am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\\n', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "aP9IQ7q8mYu8",
        "outputId": "4b00f916-f22f-4698-ad4f-9f6e827d0f0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are a helpful A trustworthy friend.</s>\\n<|user|>\\nHello, how are you doing?</s>\\n<|assistant|>\\nI'm doing well, thanks!</s>\\n<|user|>\\n\\nI am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\\n</s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Convert the formatted messages into a chat prompt that can be processed by the Hugging Face model.\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOm5ZjnQ24jm",
        "outputId": "6f133dd0-b1cf-48d6-d738-f3cf56752e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a helpful A trustworthy friend.</s>\n",
            "<|user|>\n",
            "Hello, how are you doing?</s>\n",
            "<|assistant|>\n",
            "I'm doing well, thanks!</s>\n",
            "<|user|>\n",
            "\n",
            "I am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\n",
            "</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKH257JH_6qi",
        "outputId": "46ccab31-6f28-4f87-f1dc-f61aa5968f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I'd be happy to help you! Here are five points you can work on to better understand the concept:\n",
            "\n",
            "1. Define the concept: Make sure you have a clear understanding of what the concept is about. Write down a definition in your own words.\n",
            "\n",
            "2. Find examples: Look for real-life examples that illustrate the concept. This will help you understand how it works in practice.\n",
            "\n",
            "3. Practice using it: Try applying the concept to different scenarios. The more you practice, the more comfortable you'll become with it.\n",
            "\n",
            "4. Collaborate with classmates: Discuss the concept with your classmates and see how they interpret it. This can help you gain a different perspective and clarify any misunderstandings.\n",
            "\n",
            "5. Seek help from the teacher: Don't be afraid to ask questions in class or schedule a one-on-one session with your teacher. They can provide further clarification and guide you through the concept.\n",
            "\n",
            "Good luck!\n"
          ]
        }
      ],
      "source": [
        "# Invoke the chat model with the formatted messages and get the model's response.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the content of the response from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si6NaQVggzQq"
      },
      "source": [
        "**Practice-3 :**\n",
        "Create a prompt template that takes context and a question from the user and answers the question based on the given context.\n",
        "\n",
        "Hint: Keep the context in the system message and the question in the human message.\n",
        "\n",
        "**Context:** Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled  talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\n",
        "**Question:** What awards did Aryan Kapoor win for his contributions to the entertainment industry, and in which years were they received?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I1uzhaCN1enO"
      },
      "outputs": [],
      "source": [
        "# YOUR CODES HERE for above practice exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "t4StVsQ7lQS7"
      },
      "outputs": [],
      "source": [
        "# Create a ChatPromptTemplate using a list of predefined message templates for system and human interactions.\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # The system message, providing the context for the assistant and setting expectations for the response.\n",
        "        (\"system\", \"You are a helpful assistant and know this context ```{context}``\"),\n",
        "\n",
        "        # The human message, containing the query, asking for a point-based answer and to stick to the provided context.\n",
        "        (\"human\", \" pls reply ```{question}``` in points based on the context provided. Strictly don't add extra facts and information ?\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MN6-TQaliTMT"
      },
      "outputs": [],
      "source": [
        "# Define the user's question about Aryan Kapoor's awards and the years they were received.\n",
        "question = \"\"\"What awards did Aryan Kapoor win for his contributions to the entertainment industry,\n",
        "and in which years were they received?\"\"\"\n",
        "\n",
        "# Define the context, which provides detailed information about Aryan Kapoor's achievements.\n",
        "context = \"\"\"\n",
        "Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\n",
        " prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\n",
        "  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\n",
        "  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\n",
        "   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\n",
        "   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\"\"\"\n",
        "\n",
        "# Format the chat prompt messages by inserting the context and question into the message template.\n",
        "messages = chat_template.format_messages(context=context, question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "5u6PwxIqjdfn",
        "outputId": "ae559753-6456-46f5-d7c8-03b72e404305"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nYou are a helpful assistant and know this context ```\\nMeet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\\naudiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\\n prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\\n  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\\n  Aryan\\'s captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\\n  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\\n   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\\n   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\\n``</s>\\n<|user|>\\n pls reply ```What awards did Aryan Kapoor win for his contributions to the entertainment industry,\\nand in which years were they received?``` in points based on the context provided. Strictly don\\'t add extra facts and information ?</s>\\n<|assistant|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Convert the formatted messages into a chat prompt that can be processed by the Hugging Face model.\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs-ph2ePAFb5",
        "outputId": "387262b5-db0f-4ea2-807e-7c99b18b85c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. In the year 2023, Aryan Kapoor won the prestigious Best Actor award at the National Film Awards for his outstanding performance in the movie \"Echoes of Eternity\".\n",
            "\n",
            "2. In the year 2024, Aryan Kapoor showcased his vocal talent by singing playback for the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon\".\n",
            "\n",
            "3. In 2024, Aryan Kapoor received the coveted Filmfare Critics Award for Best Actor, acknowledging his captivating screen presence in his acting roles.\n",
            "\n",
            "4. In 2025, Aryan Kapoor was conferred with the prestigious International Icon of the Year award at the Global Entertainment Awards, in recognition of his global impact and widespread admiration in the entertainment industry.\n"
          ]
        }
      ],
      "source": [
        "# Send the formatted messages to the chat model for processing and obtain the model's response.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BPY4s_hGQv-"
      },
      "source": [
        "#### **Output Parsers**\n",
        "\n",
        "Let's start with defining how we would like the LLM output to look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1nyuc_1GO6u",
        "outputId": "9af3316b-d7a2-4eee-9c83-3c34e2e9726e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# An example output format\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qW37-qdYGaOY"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ppJOtjsEGdL_"
      },
      "outputs": [],
      "source": [
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift or present for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQDimFP3Gp5a",
        "outputId": "c765270b-a282-483e-a192-52f1cec1b0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift or present for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
          ]
        }
      ],
      "source": [
        "# Import ChatPromptTemplate from LangChain Core to create structured prompts for chat models.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Creating a chat prompt template using a predefined template string (review_template).\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "\n",
        "# Print the created prompt template to check its structure.\n",
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYur071rMz6D",
        "outputId": "668e8408-e10b-431c-f279-ca357a06147c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": True,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Format the chat prompt messages by filling in the placeholder(s) in the prompt template with the actual customer review text.\n",
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "\n",
        "# Send the formatted messages to the chat model for processing and obtain the model's response.\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Print the generated response content from the chat model.\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV1eVBvyHhfY",
        "outputId": "c175319b-2ea6-41ff-e190-a2a0adf1a2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "print(type(response.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3bgzsf7pgP"
      },
      "source": [
        "#### **Parse the LLM output string into a structured data**:\n",
        "\n",
        "Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support [built-in ways to return structured output](https://python.langchain.com/docs/how_to/structured_output/), not all do.\n",
        "\n",
        "Output parsers are classes that help structure language model responses.\n",
        "\n",
        "Below we go over the main type of output parser, the `PydanticOutputParser`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8N7zx3mCoIj"
      },
      "source": [
        "[Structured output parser](https://python.langchain.com/docs/how_to/output_parser_structured/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EH0hArbY7n4t"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "from langchain_core.output_parsers import PydanticOutputParser  # Parses model output into structured data\n",
        "from pydantic import BaseModel, Field  # Used for defining structured data models\n",
        "\n",
        "# Define the structured data model for extracting product-related information\n",
        "class Product_Info(BaseModel):\n",
        "    \"\"\"Product service info.\"\"\"\n",
        "\n",
        "    # Field to check if the product was purchased as a gift\n",
        "    gift: str = Field(description=\"Was the item purchased as a gift for someone else? \"\n",
        "                                  \"Answer 'True' if yes, 'False' if not or unknown.\")\n",
        "\n",
        "    # Field to capture the number of days taken for delivery\n",
        "    delivery_days: int = Field(description=\"How many days did it take for the product to arrive? \"\n",
        "                                           \"If this information is not found, output -1.\")\n",
        "\n",
        "    # Field to extract and store sentences related to price or value as a list\n",
        "    price_value: list = Field(description=\"Extract sentences about the value or price, \"\n",
        "                                          \"and output them as a comma-separated Python list.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QzpGk67j24Ti"
      },
      "outputs": [],
      "source": [
        "# Set up a parser + inject instructions into the prompt template\n",
        "parser = PydanticOutputParser(pydantic_object = Product_Info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "sJp6QGTF3Ma7"
      },
      "outputs": [],
      "source": [
        "# Import PromptTemplate from LangChain to create structured prompts for the model\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a prompt template to guide the model's response generation\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{text}\\n\",  # Template structure\n",
        "    input_variables=[\"text\"],  # Defines 'text' as a required input variable\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},  # Inserts format instructions dynamically\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLPLDAGm9c0F",
        "outputId": "09f26c4d-14a9-4f01-b2a3-0618f63e5198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Product_Info(gift='True', delivery_days=2, price_value=['slightly more expensive'])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Create a pipeline where the formatted prompt is passed to the language model (llm)\n",
        "prompt_and_model = prompt | llm  # The \"|\" operator chains the prompt and the LLM for execution\n",
        "\n",
        "# Invoke the pipeline with a user query (customer review) to generate a structured response\n",
        "output = prompt_and_model.invoke({\"text\": customer_review})\n",
        "\n",
        "# Parse the model's output into the predefined structured format using the parser\n",
        "result = parser.invoke(output)\n",
        "\n",
        "# Display the final structured result\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3BgDcCa7tcI",
        "outputId": "c0248aea-d12c-4c96-e87c-8feec1193158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "2\n",
            "['slightly more expensive']\n"
          ]
        }
      ],
      "source": [
        "print(result.gift)\n",
        "print(result.delivery_days)\n",
        "print(result.price_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG22QIv3s_p9"
      },
      "source": [
        "**Practice-4 :** Continuing the practice-3, can you get the ouput in the below format :\n",
        "\n",
        "{'Year': 'Award}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fNQ4yrNH4x_U"
      },
      "outputs": [],
      "source": [
        "# YOUR CODES HERE for above practice exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br30bwaPKs0N"
      },
      "source": [
        "#### [**Customizing Conversational Memory**](https://python.langchain.com/docs/how_to/chatbots_memory/)\n",
        "\n",
        "LangChain can helps in building better chatbots, or have\n",
        "an LLM with more effective chats by better managing\n",
        "what it remembers from the conversation you've had so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "TBDcA56ovm_w"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "C1_WmMeZv1HV"
      },
      "outputs": [],
      "source": [
        "# Import ChatMessageHistory to manage and store conversation history\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# Import RunnableWithMessageHistory to enable conversation-aware execution of chat models\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-zGGnVncvtWt"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store chat message histories for different sessions\n",
        "store = {}\n",
        "\n",
        "# Function to retrieve or create a new chat history for a given session ID\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()  # Initialize new history if session does not exist\n",
        "    return store[session_id]  # Return the existing or new session history\n",
        "\n",
        "# Create a chat pipeline with message history tracking\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    runnable=chain,  # The main conversation chain (LLM model + prompt)\n",
        "    get_session_history=get_session_history,  # Function to retrieve chat history per session\n",
        "    input_messages_key=\"input\",  # Key in the input where the new user message is stored\n",
        "    history_messages_key=\"chat_history\",  # Key in the input where the previous chat history is stored\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "PTe4OV8Kwoat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3575df-c42e-4934-904b-58e4c5e46717"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello, James! I'm glad you decided to reach out. My name is [your assistant name], and I'm here to provide helpful information and assistance whenever and wherever you need it. Feel free to ask me any questions you have, and I'll do my best to provide accurate and insightful answers. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=73, prompt_tokens=57, total_tokens=130), 'model': '', 'finish_reason': 'stop'}, id='run-5a96edf3-f480-42cb-aaef-2954faa6561a-0')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# Invoke the chat model with message history tracking\n",
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Hi, my name is James\"},  # User's input message\n",
        "    {\"configurable\": {\"session_id\": \"user1\"}},  # Configuration containing session ID\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ALK5ZboQw53S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91327c2b-aa9c-417e-8358-57fc87cd50b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! I'm glad you decided to reach out. My name is [your assistant name], and I'm here to provide helpful information and assistance whenever and wherever you need it. Feel free to ask me any questions you have, and I'll do my best to provide accurate and insightful answers. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=73, prompt_tokens=57, total_tokens=130), 'model': '', 'finish_reason': 'stop'}, id='run-5a96edf3-f480-42cb-aaef-2954faa6561a-0')])}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "HQPxFgMEwz0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4ce737-226b-45e8-d6d6-5d86d5095dee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I don't have access to personal information or memories about prior interactions. My training data and programming only allows me to remember information that is input into my system or that I've learned through repeated interactions. However, based on our previous conversation, you did introduce yourself as James. Is there anything else I can help you with today?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=75, prompt_tokens=162, total_tokens=237), 'model': '', 'finish_reason': 'stop'}, id='run-cdcc6957-b27a-4942-937b-8fff14196dbd-0')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Invoke the chat model with message history tracking for a specific user session\n",
        "chain_with_message_history.invoke(\n",
        "    input={\"input\": \"Do you remember my name?\"},  # User's current input message\n",
        "    config={\"configurable\": {\"session_id\": \"user1\"}}  # Session configuration to maintain chat history\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "vsmS0nLgxdxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff604072-adf7-4f41-8bd4-291e31a8fdc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! I'm glad you decided to reach out. My name is [your assistant name], and I'm here to provide helpful information and assistance whenever and wherever you need it. Feel free to ask me any questions you have, and I'll do my best to provide accurate and insightful answers. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=73, prompt_tokens=57, total_tokens=130), 'model': '', 'finish_reason': 'stop'}, id='run-5a96edf3-f480-42cb-aaef-2954faa6561a-0'), HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, but I don't have access to personal information or memories about prior interactions. My training data and programming only allows me to remember information that is input into my system or that I've learned through repeated interactions. However, based on our previous conversation, you did introduce yourself as James. Is there anything else I can help you with today?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=75, prompt_tokens=162, total_tokens=237), 'model': '', 'finish_reason': 'stop'}, id='run-cdcc6957-b27a-4942-937b-8fff14196dbd-0')])}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "gbW0TdOVxef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c385e523-f2cc-4f9c-96e2-d08d2816ca09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry but I don't have access to information about your name as it's not provided during our interactions. Whenever you interact with me, you introduce yourself with your name, and I remember it for that session. However, I cannot store that information outside of our ongoing communication. Would you like me to remember your name for this session, or is this simply an experiment to see if I can do so? If you'd like me to remember your name, please let me know your name once again, and I'll make a note of it for this session.\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=120, prompt_tokens=272, total_tokens=392), 'model': '', 'finish_reason': 'stop'}, id='run-e4987768-cdd6-4330-aea6-22ce997e4e2a-0')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# Invoke the chat model with message history tracking to retrieve the user's name\n",
        "chain_with_message_history.invoke(\n",
        "    input={\"input\": \"Can you tell me what is my name?\"},  # User's current query asking for their name\n",
        "    config={\"configurable\": {\"session_id\": \"user1\"}}  # Session configuration to ensure chat history for \"user1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Wb2HR0MAvm3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25c8150-29e8-4748-d2a3-f9e9fae2e26e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! I'm glad you decided to reach out. My name is [your assistant name], and I'm here to provide helpful information and assistance whenever and wherever you need it. Feel free to ask me any questions you have, and I'll do my best to provide accurate and insightful answers. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=73, prompt_tokens=57, total_tokens=130), 'model': '', 'finish_reason': 'stop'}, id='run-5a96edf3-f480-42cb-aaef-2954faa6561a-0'), HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, but I don't have access to personal information or memories about prior interactions. My training data and programming only allows me to remember information that is input into my system or that I've learned through repeated interactions. However, based on our previous conversation, you did introduce yourself as James. Is there anything else I can help you with today?\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=75, prompt_tokens=162, total_tokens=237), 'model': '', 'finish_reason': 'stop'}, id='run-cdcc6957-b27a-4942-937b-8fff14196dbd-0'), HumanMessage(content='Can you tell me what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry but I don't have access to information about your name as it's not provided during our interactions. Whenever you interact with me, you introduce yourself with your name, and I remember it for that session. However, I cannot store that information outside of our ongoing communication. Would you like me to remember your name for this session, or is this simply an experiment to see if I can do so? If you'd like me to remember your name, please let me know your name once again, and I'll make a note of it for this session.\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=120, prompt_tokens=272, total_tokens=392), 'model': '', 'finish_reason': 'stop'}, id='run-e4987768-cdd6-4330-aea6-22ce997e4e2a-0')])}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSbCSUOfxXa-"
      },
      "source": [
        "### **II. mistralai/Mistral-7B-Instruct-v0.2**\n",
        "\n",
        "Note that you need to ask for access before using this model. Go to https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 and click on `Agree and access repository`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "h_733Aai1RG3"
      },
      "outputs": [],
      "source": [
        "# Importing HuggingFaceEndpoint from langchain_huggingface to interact with Hugging Face models via an API endpoint\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_XZllnjBzqw0"
      },
      "outputs": [],
      "source": [
        "question = \"How to learn programing? Give 5 examples. \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "bNrWpxyu5yxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393ef064-adea-4f25-cdd1-67da7dfe6360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "# Set the Hugging Face model repository ID\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Model identifier for Mistral-7B\n",
        "\n",
        "# Define additional model parameters, including max token length and authentication token\n",
        "model_kwargs = {\n",
        "    \"max_length\": 128,  # Maximum token length for generated output\n",
        "    \"token\": os.environ[\"HF_TOKEN\"]  # Hugging Face API token for authentication, stored in environment variables\n",
        "}\n",
        "\n",
        "# Initialize the HuggingFaceEndpoint to interact with the model hosted on Hugging Face\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,  # Model repo ID to specify which model to use\n",
        "    task=\"text-generation\",  # Task to perform, here it's text generation (common for LLMs)\n",
        "    temperature=0.5,  # Temperature parameter controls the randomness of the output (higher value = more random)\n",
        "    model_kwargs=model_kwargs  # Pass the additional model parameters (max length, token)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "bXJMBgfy0A5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5466b9e8-6c6d-4d18-85c5-a80c61a33a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Learn a programming language: Python is a great language for beginners because it's easy to read and write, and it has a wide range of applications. You can start by learning the basics of Python, such as variables, data types, and control structures, and then move on to more advanced topics like functions, modules, and object-oriented programming. There are many online resources available to help you learn Python, such as Codecademy, W3Schools, and the official Python documentation. 2. Build a project: Once you have a basic understanding of a programming language, it's a good idea to start working on a project. This will give you practical experience and help you apply what you've learned. Some project ideas for beginners include creating a simple calculator, building a to-do list, or making a basic website. You can find many tutorials and resources online to help you get started with your project. 3. Join a coding community: There are many online communities where you can connect with other programmers and learn from their experiences. Some popular coding communities include Stack Overflow, GitHub, and Reddit's r/learnprogramming subreddit. These communities can be a great place to ask questions, get feedback on your code, and learn about new tools and technologies. 4. Read books: There are many books available that can help you learn programming. Some popular programming books for beginners include \"Automate the Boring Stuff with Python\" by Al Sweigart, \"Python Crash Course\" by Eric Matthes, and \"Eloquent JavaScript\" by Marijn Haverbeke. These books can provide a more in-depth understanding of programming concepts and help you develop your skills. 5. Practice coding regularly: Like any other skill, the more you practice programming, the better you'll become. Try to set aside some time each day to practice coding, even if it's just for a few minutes. You can use websites like LeetCode, HackerRank, and Exercism to find coding challenges and problems to solve. The more you practice, the more comfortable you'll become with programming concepts and the faster you'll be able to solve problems.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Use the Hugging Face model endpoint to generate a response based on the input question\n",
        "response = llm.invoke(question)  # Pass the user's question to the model for text generation\n",
        "print(response)  # Output the generated response from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmRI-p9fp0SR"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "**Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "xiZNTCsGNpPC"
      },
      "outputs": [],
      "source": [
        "# Importing message types from langchain.schema to represent different types of messages in a conversation\n",
        "from langchain.schema import (\n",
        "    HumanMessage,  # Represents a message from the user (human)\n",
        "    SystemMessage,  # Represents a message from the system, providing context or instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "WnF-Qc_FrTZV"
      },
      "outputs": [],
      "source": [
        "# Importing the ChatPromptTemplate class from langchain_core.prompts to create and format chat-based prompts\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "SHJWqMg3yxa_"
      },
      "outputs": [],
      "source": [
        "template_s = \"\"\"You are a {style1}.\\\n",
        "Tell me  {count} facts about {event_or_place}.```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "wHucwPZ7yVPF"
      },
      "outputs": [],
      "source": [
        "# Creating a ChatPromptTemplate using a template string, where 'template_s' is a predefined prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(template_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "jH_lqJRbzaHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20506b38-2119-4331-a17c-6b9574e6704c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['count', 'event_or_place', 'style1'], input_types={}, partial_variables={}, template='You are a {style1}.Tell me  {count} facts about {event_or_place}.```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "# Accessing the prompt of the first message in the ChatPromptTemplate\n",
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "fDyBeGD8zej9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cfb451-ead2-4ae3-d18b-b8ee74ea8e6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['count', 'event_or_place', 'style1']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# Accessing the input variables of the first message's prompt in the ChatPromptTemplate\n",
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "QXUiH0sszkUe"
      },
      "outputs": [],
      "source": [
        "# Formatting the messages using the prompt_template with specific values for placeholders\n",
        "user_messages = prompt_template.format_messages(\n",
        "    style1=\"knowledgeable historian\",  # Replacing the 'style1' placeholder with the value \"knowledgeable historian\"\n",
        "    count=5,                           # Replacing the 'count' placeholder with the value 5\n",
        "    event_or_place=\"Tajmahal\"          # Replacing the 'event_or_place' placeholder with the value \"Tajmahal\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qP62G5bz0KBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afa33e3-4ff0-4cf0-f485-a6b3eac728ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='You are a knowledgeable historian.Tell me  5 facts about Tajmahal.```\\n', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "user_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Q5IfurZcyYt8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cb032a1b-c1ed-4286-f43f-aa20ac7f9405"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mistralai/Mistral-7B-Instruct-v0.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "# Importing ChatHuggingFace from langchain_huggingface to interface with Hugging Face models\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "# Creating an instance of ChatHuggingFace using a previously defined language model (llm)\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# Accessing the model ID associated with the chat_model instance\n",
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ybxEq_9JqywX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ee4e4c6b-9369-4f0c-c977-25787c7f231d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [INST] You are a knowledgeable historian.Tell me  5 facts about Tajmahal.```\\n [/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# Converting the user messages to a chat-friendly format for the chat_model\n",
        "chat_model._to_chat_prompt(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "SYI0aD8Hsty_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34413fdf-cd0c-4c0d-a4d7-c5eb02a0bf38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. The Taj Mahal is located in Agra, India, and was completed in 1653. It was built by Mughal Emperor Shah Jahan as a mausoleum for his wife Mumtaz Mahal.\n",
            "2. The Taj Mahal is considered one of the Seven Wonders of the World and is a UNESCO World Heritage Site. It is renowned for its white marble exterior and intricate craftsmanship, which includes calligraphy and inlaid semi-precious stones.\n",
            "3. The Taj Mahal complex includes not only the mausoleum but also a mosque, a guest house, and a large reflecting pool. The complex is symmetrical, with the entrance to the mosque mirroring the entrance to the mausoleum across the pool.\n",
            "4. Contrary to popular belief, the Taj Mahal is not a perfect symmetric structure. A careful examination reveals slight differences between the right and left sides of the building, including the size and position of some of the elements.\n",
            "5. The Taj Mahal was built using an estimated 20,000 workers, who came from all over India and Central Asia. The construction took over 20 years to complete, and the project required the transport of large quantities of marble and other materials from distant locations.\n"
          ]
        }
      ],
      "source": [
        "# Invoking the chat model with user messages to generate a response\n",
        "response = chat_model.invoke(user_messages)\n",
        "\n",
        "# Printing the content of the response generated by the chat model\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wakjVUVZp2YH"
      },
      "source": [
        "**Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "jiFDpiyENyOS"
      },
      "outputs": [],
      "source": [
        "# Creating a list of messages with a single human message as input\n",
        "messages = [HumanMessage(content=\"How to learn programming? give 5 points\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "6SO2qIrju7rc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c6a710b1-9283-43af-caab-adfc3e5e0b26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [INST] How to learn programming? give 5 points [/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "# Converting the list of messages to the appropriate chat format for the chat model\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "jEhKW2msODT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1799877d-b37d-4e9d-cb03-b808efeefbb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. Start with the Basics: Begin by learning the fundamental concepts of programming such as variables, loops, conditional statements, functions, and data structures. Pick a programming language that interests you and stick with it until you have a strong foundation. Some good options for beginners include Python, JavaScript, or C++.\n",
            "\n",
            "2. Practice Regularly: Consistent practice is crucial when learning programming. Try to write code every day, even if it's just for a few minutes. You can find plenty of free resources online, such as coding exercises, projects, and tutorials. Each time you write code, you'll learn something new and build upon your existing knowledge.\n",
            "\n",
            "3. Learn the Syntax and Semantics: Understand the rules and structure of the programming language you're using. Familiarize yourself with the syntax (the rules for writing code) and semantics (the meaning behind the code). This will help you read, write, and understand code more effectively.\n",
            "\n",
            "4. Collaborate and Learn from Others: Join online communities, forums, and social media groups for programmers. Share your code and seek feedback from others. Learn from more experienced developers and their projects. This not only helps you grow as a programmer, but it also broadens your network and makes the learning process more enjoyable.\n",
            "\n",
            "5. Build Real Projects: Instead of just coding exercises, focus on building real projects that solve a problem or address a need. This will give you a better understanding of programming concepts and help you develop practical skills that can be applied in the real world. Additionally, completed projects can serve as valuable additions to your portfolio.\n"
          ]
        }
      ],
      "source": [
        "# Sending the formatted messages to the chat model and generating a response\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "# Printing the content of the response generated by the chat model\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaaC7-gu_KOK"
      },
      "source": [
        "### **III.** **[Llama2](https://ai.meta.com/llama/)** ***(Optional)***\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        ">For using this model you have to click `Download models` link available in [this](https://ai.meta.com/llama/) reference which re-direct to a **form for request**. It may take 1 hour to 2 days to get the **approval** for usage of this model through HuggingFace. You will get an email for the same.\n",
        "\n",
        ">Once the request is approved, connect to **GPU runtime** for below steps. Also, you need to provide your HF api key/access token.\n",
        "\n",
        "Trying Llama2-2-7b model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "HbzbpWIK_J4T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# This is a Jupyter notebook magic command that captures the output of the cell,\n",
        "# preventing it from being printed in the notebook.\n",
        "\n",
        "!pip install -q transformers accelerate langchain xformers bitsandbytes\n",
        "# Using pip to install the following Python packages:\n",
        "# - transformers: A library for working with transformer models like GPT, BERT, etc.\n",
        "# - accelerate: A library to speed up the training and inference of large models efficiently.\n",
        "# - langchain: A framework for building applications using large language models (LLMs).\n",
        "# - xformers: Tools and optimizations for efficient memory usage and performance in transformers.\n",
        "# - bitsandbytes: A library for running models with low-bit precision to improve memory usage and speed.\n",
        "# The '-q' flag ensures the installation process runs quietly without verbose output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zW69WOWSzwkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95ed335-97dd-496a-bbec-1a1c8f897f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HuggingFace access token: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "# Enter your HuggingFace access token when prompted\n",
        "\n",
        "import os  # Importing the 'os' module for interacting with the operating system\n",
        "from getpass import getpass  # Importing 'getpass' to securely input the HuggingFace token without displaying it\n",
        "\n",
        "# Prompt the user to input their HuggingFace access token without showing the input\n",
        "pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "# Setting the HuggingFace token as environment variables to be used in API requests\n",
        "os.environ[\"HF_TOKEN\"] = pass_token  # Storing the token as 'HF_TOKEN' in the environment\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token  # Storing the token as 'HUGGINGFACEHUB_API_TOKEN'\n",
        "\n",
        "# Delete the pass_token variable for security reasons to ensure it doesn't remain in memory\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw7oXd2pAE1f"
      },
      "source": [
        "## Initializing the Hugging Face Pipeline\n",
        "\n",
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `meta-llama/Llama-2-7b-chat-hf`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "--40_VMGJBEz"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "# Importing 'cuda' from PyTorch to interact with GPU for acceleration (e.g., checking for available CUDA devices, using the GPU for tensor operations).\n",
        "# Importing 'bfloat16' from PyTorch, a 16-bit floating point format often used to save memory and speed up computation, especially in deep learning.\n",
        "\n",
        "import transformers\n",
        "# Importing the 'transformers' library from Hugging Face, which provides tools for working with transformer models like GPT, BERT, and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "h49z-C1RJC9D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a7d3f3f0-7832-4bb3-c317-ff7a4c9aebff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "# Define the model ID for the pre-trained model you want to use from Hugging Face\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "# 'meta-llama/Llama-2-7b-chat-hf' is the ID of a pre-trained Llama-2 model hosted on Hugging Face.\n",
        "# This model is fine-tuned for chat applications.\n",
        "\n",
        "# Determine which device (GPU or CPU) to use for model inference\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "# If a CUDA-enabled GPU is available, use the first available GPU (cuda:0).\n",
        "# If CUDA is not available, use the CPU ('cpu').\n",
        "device  # This stores the device configuration for later use when loading the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install fastai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xGmDQHTUoejD",
        "outputId": "2ad5cffb-cb24-4dde-b0b7-d1247340191c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.5.1+cu124 (from versions: 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6, 2.1.2, 2.1.2+cpu, 2.1.2+cpu.cxx11.abi, 2.1.2+cu118, 2.1.2+cu121, 2.1.2+cu121.with.pypi.cudnn, 2.1.2+rocm5.5, 2.1.2+rocm5.6, 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7, 2.2.2, 2.2.2+cpu, 2.2.2+cpu.cxx11.abi, 2.2.2+cu118, 2.2.2+cu121, 2.2.2+rocm5.6, 2.2.2+rocm5.7, 2.3.0, 2.3.0+cpu, 2.3.0+cpu.cxx11.abi, 2.3.0+cu118, 2.3.0+cu121, 2.3.0+rocm5.7, 2.3.0+rocm6.0, 2.3.1, 2.3.1+cpu, 2.3.1+cpu.cxx11.abi, 2.3.1+cu118, 2.3.1+cu121, 2.3.1+rocm5.7, 2.3.1+rocm6.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.5.1+cu124\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: fastai in /usr/local/lib/python3.11/dist-packages (2.7.18)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from fastai) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastai) (24.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.11/dist-packages (from fastai) (1.7.29)\n",
            "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.11/dist-packages (from fastai) (0.20.1+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from fastai) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fastai) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from fastai) (2.32.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from fastai) (6.0.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from fastai) (11.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fastai) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fastai) (1.13.1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.11/dist-packages (from fastai) (3.7.5)\n",
            "Collecting torch<2.6,>=1.10 (from fastai)\n",
            "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (12.4.127)\n",
            "Collecting triton==3.1.0 (from torch<2.6,>=1.10->fastai)\n",
            "  Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=1.10->fastai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.6,>=1.10->fastai) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fastai) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fastai) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fastai) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fastai) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.17.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4->fastai) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai) (0.1.2)\n",
            "Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
            "Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Installing collected packages: triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0\n",
            "    Uninstalling torch-2.6.0:\n",
            "      Successfully uninstalled torch-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.5.1 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "triton"
                ]
              },
              "id": "5846853f9c3a4a87b43198c3d1170b43"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers accelerate langchain xformers bitsandbytes --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGN-gRVTnIYA",
        "outputId": "5ba032a0-9b37-4bfe-b636-094457397160"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "T2Cp6jnomS0S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "b407980e-ae79-4a17-c665-602fc7acf87c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_image_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision::nms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0muse_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0muse_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m_register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_to_register\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registration_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_library/fake_impl.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, func, source)\u001b[0m\n\u001b[1;32m     30\u001b[0m             )\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_has_kernel_for_dispatch_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: operator torchvision::nms does not exist",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-9c4c3e5fd499>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Set up a text-generation pipeline using the specified model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m pipeline = transformers.pipeline(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Specifies the type of task (text generation in this case)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# The model to use for text generation (in this case, Llama-2 model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1849\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1851\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1852\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1863\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1866\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer for the model specified by 'model_id' from Hugging Face\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "# 'AutoTokenizer' automatically selects the appropriate tokenizer based on the model provided.\n",
        "# The tokenizer is responsible for converting text input into token IDs that the model can understand.\n",
        "\n",
        "# Set up a text-generation pipeline using the specified model and tokenizer\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",  # Specifies the type of task (text generation in this case)\n",
        "    model=model_id,  # The model to use for text generation (in this case, Llama-2 model)\n",
        "    tokenizer=tokenizer,  # Tokenizer to convert input text into tokens\n",
        "    torch_dtype=bfloat16,  # Use 'bfloat16' precision for tensor computations to reduce memory usage and speed up computations\n",
        "    trust_remote_code=True,  # Trust remote code when loading model weights and configurations\n",
        "    device_map=\"auto\",  # Automatically map the model layers to available devices (CPU/GPU)\n",
        "    max_length=1000,  # Maximum length of the generated text (number of tokens)\n",
        "    do_sample=True,  # Enable sampling for text generation (instead of deterministic outputs)\n",
        "    top_k=10,  # Limits the sampling to the top 10 most probable next tokens to ensure diversity\n",
        "    num_return_sequences=1,  # Number of generated sequences to return\n",
        "    eos_token_id=tokenizer.eos_token_id  # End-of-sequence token ID, so generation stops when this token is produced\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "8n7MjpX4nM-r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "8ebf6e19-b81e-407a-ab42-b93c483dfb3f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-6c9c99d3669e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pass the input prompt to the pipeline and generate text based on the given prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How to learn programming?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# The pipeline processes the input prompt (\"How to learn programming?\") and generates text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The result is stored in the variable 'res'. The pipeline returns a list of generated sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "# Pass the input prompt to the pipeline and generate text based on the given prompt\n",
        "res = pipeline(\"How to learn programming?\")\n",
        "# The pipeline processes the input prompt (\"How to learn programming?\") and generates text.\n",
        "# The result is stored in the variable 'res'. The pipeline returns a list of generated sequences.\n",
        "\n",
        "# Print the generated text from the first sequence\n",
        "print(res[0][\"generated_text\"])\n",
        "# The generated text is accessed from the first result in the list (res[0]).\n",
        "# 'generated_text' is the key that contains the model's output for the generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqImT9RAdAg"
      },
      "source": [
        "#### **Now implementing with LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "hLkLetCxWF-t"
      },
      "outputs": [],
      "source": [
        "# Install the LangChain library, a framework for building language model-based applications\n",
        "!pip -q install langchain\n",
        "# The '-q' flag suppresses unnecessary output, making the installation process less verbose.\n",
        "# LangChain is useful for building chains of various language model components like prompts, tools, and memory.\n",
        "\n",
        "# Install the langchain_community library, which contains community-supported modules for LangChain\n",
        "!pip -q install langchain_community\n",
        "# This library includes additional modules contributed by the LangChain community to extend its functionality."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Define the model ID and device\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Load the tokenizer and create the pipeline\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-vCujAAwsDfz",
        "outputId": "52b577b9-ad76-46cb-8d81-d8a5b29fb168"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0\n",
            "Uninstalling torch-2.6.0:\n",
            "  Successfully uninstalled torch-2.6.0\n",
            "Found existing installation: torchvision 0.20.1+cu124\n",
            "Uninstalling torchvision-0.20.1+cu124:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu124\n",
            "Found existing installation: torchaudio 2.5.1+cu124\n",
            "Uninstalling torchaudio-2.5.1+cu124:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "5adaf11693a94cd79c9edfc881dc4fa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_processing_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_image_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregister_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roi_align\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_roi_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mget_meta_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-ab083805549e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the tokenizer and create the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m pipeline = transformers.pipeline(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1849\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1851\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1852\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1863\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1866\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\npartially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "h71jByoBAffB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "51e789eb-359e-464c-f29c-c37d7bbdee25"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-c0b3f9b7e69c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create an LLM instance by wrapping the HuggingFace pipeline inside LangChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The pipeline processes the input data and generates responses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFacePipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# 'pipeline': The pre-configured HuggingFace pipeline used to generate responses based on input prompts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 'model_kwargs': Additional keyword arguments passed to the model, in this case, setting the 'temperature' to 0.7.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "# Import HuggingFacePipeline from langchain.llms\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Create an LLM instance by wrapping the HuggingFace pipeline inside LangChain\n",
        "# The pipeline processes the input data and generates responses.\n",
        "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.7})\n",
        "# 'pipeline': The pre-configured HuggingFace pipeline used to generate responses based on input prompts.\n",
        "# 'model_kwargs': Additional keyword arguments passed to the model, in this case, setting the 'temperature' to 0.7.\n",
        "# 'temperature': A parameter that controls the randomness of the output (higher values lead to more randomness)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tr-YGpvnr0X"
      },
      "outputs": [],
      "source": [
        "# Invoke the HuggingFace model wrapped inside the LangChain LLM with the query \"How to learn programming?\"\n",
        "# The 'invoke' method runs the input query through the model pipeline and generates a response.\n",
        "print(llm.invoke(\"How to learn programming?\"))\n",
        "# This will output the response generated by the model to the prompt \"How to learn programming?\".\n",
        "# The model will process the query and provide an answer based on its training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prWXot8wcKyS"
      },
      "source": [
        "#### **Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "7DTl7I1HaOJK"
      },
      "outputs": [],
      "source": [
        "# Importing ChatPromptTemplate from langchain.prompts\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "# This class allows for creating templates for chat-based prompts. You can define the structure of messages in the chat,\n",
        "# and then dynamically replace parts of it based on inputs or parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "mqU1EAehaOJP"
      },
      "outputs": [],
      "source": [
        "# Define a multi-line string template for a chat prompt using placeholders for style and text\n",
        "template_s = \"\"\"Reply the answer\n",
        "like {style1}.\n",
        "text: ```{text1}```\n",
        "\"\"\"\n",
        "# This string will be used as a template, where:\n",
        "# {style1}: Placeholder for the style in which the answer should be given (e.g., \"knowledgeable historian\").\n",
        "# {text1}: Placeholder for the text or query to which the model will respond."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "YK6fMe1paOJP"
      },
      "outputs": [],
      "source": [
        "# Create a ChatPromptTemplate instance using the previously defined template\n",
        "prompt_template = ChatPromptTemplate.from_template(template_s)\n",
        "# This initializes the ChatPromptTemplate with the string 'template_s' as its format.\n",
        "# The template will later allow dynamic substitution of the placeholders {style1} and {text1}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "R5AlexzxaOJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d54782-d7da-4fd4-920a-4f5c43982c4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style1', 'text1'], input_types={}, partial_variables={}, template='Reply the answer\\nlike {style1}.\\ntext: ```{text1}```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "# Access the first message in the prompt template and retrieve its prompt string\n",
        "prompt_template.messages[0].prompt\n",
        "# This accesses the first message in the 'messages' list of the ChatPromptTemplate,\n",
        "# and retrieves the actual prompt string (i.e., the template structure defined earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "bJa3nBfbaOJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124dfbf0-e732-4f3d-987a-5b9473a8a403"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style1', 'text1']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "# Access the input variables that are used in the prompt template\n",
        "prompt_template.messages[0].prompt.input_variables\n",
        "# This retrieves the list of input variables (placeholders) used in the first message's prompt template.\n",
        "# In this case, the input variables are {style1} and {text1}, which are placeholders for dynamic content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "dYlQrFsnaOJQ"
      },
      "outputs": [],
      "source": [
        "# Define the style to be used in the prompt template\n",
        "style = \"\"\"trustworthy friend\"\"\"\n",
        "# This assigns the string \"trustworthy friend\" to the variable 'style'.\n",
        "# This value will be used as the persona or tone in which the response should be framed in the prompt template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "D9UtT3reaOJQ"
      },
      "outputs": [],
      "source": [
        "# Define the query to be used in the prompt template\n",
        "query = \"\"\"\n",
        "I am not able to understand the concept taught in class.\n",
        "Could you please suggest something?\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\"\n",
        "# This variable 'query' holds the question or request that will be input to the language model.\n",
        "# It's a message asking for help with understanding a concept and requesting 5 points to work on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "AUB4HHQTaOJQ"
      },
      "outputs": [],
      "source": [
        "# Format the messages using the prompt template, filling in the placeholders\n",
        "user_messages = prompt_template.format_messages(\n",
        "    style1=style,   # Provide the style (e.g., \"trustworthy friend\")\n",
        "    text1=query     # Provide the user query (e.g., the request for help with 5 points)\n",
        ")\n",
        "# This line fills the placeholders {style1} and {text1} in the prompt template with the defined values\n",
        "# of 'style' (persona) and 'query' (the actual question). The resulting 'user_messages' will be used\n",
        "# to generate the final input for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "6r6WC6OZaOJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786077c1-2c42-46d8-c455-ba3e27c9b14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Reply the answer\\nlike trustworthy friend.\\ntext: ```\\nI am not able to understand the concept taught in class.\\nCould you please suggest something?\\nI need your help. Give 5 points to work on.\\n```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "# Print the first formatted message from the user_messages list\n",
        "print(user_messages[0])\n",
        "# This will output the first message in the 'user_messages' list after formatting.\n",
        "# The output will be a string where the placeholders {style1} and {text1} have been replaced by the values of 'style' and 'query'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "w-zPBqhKaOJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da27ec9c-1a78-4e78-a224-6f2f70f92cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "llm_response = llm.invoke(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "2sMDbkrmoIs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5f6ad6-6509-40c1-c646-9f04658e6ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: I'm here to help! Let's break down your study challenge into manageable tasks. Here are five points to focus on:\n",
            "\n",
            "1. **Review your notes**: Start by re-reading your notes from the class. Try to identify the key points and any areas that you found confusing.\n",
            "\n",
            "2. **Read the textbook**: Go through the relevant chapters in your textbook. Sometimes, the textbook might explain the concept in a different way that could help clarify your understanding.\n",
            "\n",
            "3. **Online resources**: Utilize online resources like video lectures, tutorials, or websites that explain the concept. Websites like Khan Academy, MIT OpenCourseWare, or YouTube channels dedicated to your subject can be very helpful.\n",
            "\n",
            "4. **Practice problems**: Try to solve problems related to the concept. This will help you understand the application of the concept and reinforce your learning. If you're stuck, don't hesitate to look up the solutions.\n",
            "\n",
            "5. **Discuss with peers or the instructor**: Reach out to your classmates or your instructor. They might be able to explain the concept in a way that resonates with you. If you're comfortable, you could also ask for a brief meeting with your instructor during their office hours.\n",
            "\n",
            "Remember, it's normal to struggle with understanding new concepts. The key is to keep trying and not give up. If you're still having trouble, consider seeking additional help like tutoring services.\n",
            "\n",
            "You can do this! I believe in your ability to grasp the concept.\n"
          ]
        }
      ],
      "source": [
        "print(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjQyyi4nR2n"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Which of the following prompt techniques in LangChain allows flexible templated prompts that are suitable for better describing the role and content? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Both\" #@param [\"\", \"PromptTemplate\", \"ChatPromptTemplate\", \"Both\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"Nothing much\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5c7153-8412-4801-e233-febbd3452d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 5473\n",
            "Date of submission:  18 Feb 2025\n",
            "Time of submission:  11:53:25\n",
            "View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}